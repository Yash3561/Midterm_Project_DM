{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnHDEEKVRy34",
        "outputId": "bf4e556b-bc0c-47d1-8602-3002199bca11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "\n",
            "Welcome to the Association Rule Mining Program!\n",
            "\n",
            "====================================================================================================\n",
            "====================================================================================================\n",
            "\n",
            "Choose a supermarket for the analysis:\n",
            "1. Amazon\n",
            "2. Target\n",
            "3. Walmart\n",
            "4. Aldi\n",
            "5. Costco\n",
            "6. Provide a custom CSV dataset for the analysis\n",
            "7. Exit from program\n",
            "\n",
            "Enter your choice between 1 and 7: 1\n",
            "Enter the minimum support percentage (1-100): 50\n",
            "Enter the minimum confidence percentage (1-100): 50\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "Frequent Itemsets:\n",
            "\n",
            "-----------------------------------------\n",
            "|Itemset                        |Support|\n",
            "-----------------------------------------\n",
            "|('Books',)                     | 55.00%|\n",
            "|('Clothes',)                   | 55.00%|\n",
            "-----------------------------------------\n",
            "\n",
            "\n",
            "* No rules found with the specified support and confidence.\n",
            "* Please enter proper inputs again\n",
            "\n",
            "====================================================================================================\n",
            "====================================================================================================\n",
            "\n",
            "Choose a supermarket for the analysis:\n",
            "1. Amazon\n",
            "2. Target\n",
            "3. Walmart\n",
            "4. Aldi\n",
            "5. Costco\n",
            "6. Provide a custom CSV dataset for the analysis\n",
            "7. Exit from program\n",
            "\n",
            "Enter your choice between 1 and 7: 1\n",
            "Enter the minimum support percentage (1-100): 20\n",
            "Enter the minimum confidence percentage (1-100): 80\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "Frequent Itemsets:\n",
            "\n",
            "-----------------------------------------\n",
            "|Itemset                        |Support|\n",
            "-----------------------------------------\n",
            "|('Amazon Echo',)               | 35.00%|\n",
            "|('Books',)                     | 55.00%|\n",
            "|('Clothes',)                   | 55.00%|\n",
            "|('Fire TV Stick',)             | 30.00%|\n",
            "|('Kindle',)                    | 30.00%|\n",
            "|('Laptop',)                    | 30.00%|\n",
            "|('Phone Charger',)             | 40.00%|\n",
            "|('Shampoo',)                   | 25.00%|\n",
            "|('Wireless Headphones',)       | 30.00%|\n",
            "|('Clothes', 'Amazon Echo')     | 20.00%|\n",
            "|('Laptop', 'Amazon Echo')      | 20.00%|\n",
            "|('Clothes', 'Books')           | 40.00%|\n",
            "|('Fire TV Stick', 'Books')     | 20.00%|\n",
            "|('Laptop', 'Books')            | 20.00%|\n",
            "|('Clothes', 'Fire TV Stick')   | 20.00%|\n",
            "|('Clothes', 'Kindle')          | 25.00%|\n",
            "|('Clothes', 'Laptop')          | 20.00%|\n",
            "|('Clothes', 'Laptop', 'Books') | 20.00%|\n",
            "-----------------------------------------\n",
            "====================================================================================================\n",
            "\n",
            "Rules generated using Brute Force Algorithm:\n",
            "\n",
            "Rule 1: ('Kindle',) -> ('Clothes',) has Support: 25.00% and Confidence: 83.33%\n",
            "\n",
            "Rule 2: ('Clothes', 'Laptop') -> ('Books',) has Support: 20.00% and Confidence: 100.00%\n",
            "\n",
            "Rule 3: ('Laptop', 'Books') -> ('Clothes',) has Support: 20.00% and Confidence: 100.00%\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "Rules generated using Apriori Algorithm:\n",
            "\n",
            "Rule 1: ('Kindle',) -> ('Clothes',) has Support: 25.00% and Confidence: 83.33%\n",
            "\n",
            "Rule 2: ('Clothes', 'Laptop') -> ('Books',) has Support: 20.00% and Confidence: 100.00%\n",
            "\n",
            "Rule 3: ('Laptop', 'Books') -> ('Clothes',) has Support: 20.00% and Confidence: 100.00%\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "Rules generated using FP-Growth Algorithm:\n",
            "\n",
            "Rule 1: ('Kindle',) -> ('Clothes',) has Support: 25.00% and Confidence: 83.33%\n",
            "\n",
            "Rule 2: ('Clothes', 'Laptop') -> ('Books',) has Support: 20.00% and Confidence: 100.00%\n",
            "\n",
            "Rule 3: ('Laptop', 'Books') -> ('Clothes',) has Support: 20.00% and Confidence: 100.00%\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "Comparison of Rules Generated by the Algorithms:\n",
            "\n",
            "All three algorithms generated the same rules.\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "Comparison of the execution times of algorithm:\n",
            "\n",
            "The Fastest algorithm is: FP-Growth Algorithm with an execution time of 0.0040 seconds.\n",
            "\n",
            "The Intermediate algorithm is: Apriori Algorithm with an execution time of 0.0071 seconds.\n",
            "\n",
            "The Slowest algorithm is: Brute Force Algorithm with an execution time of 0.0120 seconds.\n",
            "\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Import libraries for data processing and association rule mining.\n",
        "from mlxtend.frequent_patterns import association_rules, apriori, fpgrowth  # For implementing association rule mining algorithms.\n",
        "from mlxtend.preprocessing import TransactionEncoder  # For converting transaction lists into a format suitable for analysis.\n",
        "import pandas as pd  # For data manipulation and analysis using DataFrames.\n",
        "import itertools  # For generating combinations and permutations of items.\n",
        "import time  # For measuring the execution time of algorithms.\n",
        "import warnings # For fixing the deprecation warnings.\n",
        "\n",
        "def dataset_loading(file_path):  # Function for loading the dataset from a CSV file and process the transactions.\n",
        "    try:\n",
        "        record = pd.read_csv(file_path)  # Reading the CSV file using read_csv() method of pandas library.\n",
        "        transactions = []\n",
        "        for t in record['Transaction Details']:  # 'Transaction Details' is the column containing all transactions in the CSV file.\n",
        "            items = [item.strip() for item in t.split(',')]  # Split each transaction into individual items, stripping spaces.\n",
        "            transactions.append(items)  # Append the processed transaction (list of items) to the transactions list.\n",
        "        return transactions\n",
        "    except Exception as e:\n",
        "        print(f\"\\n* Error loading dataset: {e}\")# Handling error if any occurs in loading dataset and returns an empty list.\n",
        "        return []\n",
        "    except FileNotFoundError:\n",
        "        print(f\"\\n* File not found: {file_path}\")\n",
        "        return []\n",
        "    except pd.errors.ParserError:\n",
        "        print(f\"\\n* Error parsing the file: {file_path}\")\n",
        "        return []\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"\\n* No data found in the file.\")\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "def get_all_itemsets(transactions):\n",
        "    # Step 1: Extract unique items from all transactions using only for loops\n",
        "    items = set()\n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            items.add(item)  # Add item to the set (to ensure uniqueness)\n",
        "\n",
        "    # Convert the set to a list for easier manipulation\n",
        "    items = list(items)\n",
        "\n",
        "    # Step 2: Generate all itemsets using for loops\n",
        "    all_itemsets = []\n",
        "\n",
        "    # Helper function to generate combinations recursively\n",
        "    def generate_combinations(current_set, idx):\n",
        "        # If there is any combination, add it to the list\n",
        "        if current_set:\n",
        "            all_itemsets.append(tuple(current_set))  # Convert the set to a tuple and add it to the list\n",
        "\n",
        "        # Generate further combinations\n",
        "        for i in range(idx, len(items)):\n",
        "            generate_combinations(current_set + [items[i]], i + 1)\n",
        "\n",
        "    # Start the recursion with an empty list and index 0\n",
        "    generate_combinations([], 0)\n",
        "\n",
        "    return all_itemsets\n",
        "\n",
        "\n",
        "\n",
        "def support_count(itemset, transactions):  # Function to calculate the support of an itemset in the transactions.\n",
        "    count = sum(1 for t in transactions if set(itemset).issubset(set(t)))  # Counts the transactions that contain the itemset.\n",
        "    return count / len(transactions)  # Since, support is: relevant transactions divided by total transactions.\n",
        "\n",
        "\n",
        "\n",
        "def brute_force_rules(transactions, min_support, min_confidence):  # Function to generate association rules using the brute-force algorithm.\n",
        "    rules = []\n",
        "    all_itemsets = get_all_itemsets(transactions)  # Get all possible itemsets using get_all_itemsets(transactions) function.\n",
        "    if not all_itemsets:\n",
        "        print(\"* Can't generate rules as no itemsets are found!\")\n",
        "        return rules  # Returns the function with empty list of rules.\n",
        "    for itemset in all_itemsets:\n",
        "        support = support_count(itemset, transactions)  # Calculates support for the itemset.\n",
        "        if support >= min_support:\n",
        "            for i in range(1, len(itemset)):\n",
        "                for conditions in itertools.combinations(itemset, i):  # Use 'conditions' for the antecedents of the rules.\n",
        "                    results = tuple(set(itemset) - set(conditions))  # Use 'results' for the consquents of the rules.\n",
        "                    if results:\n",
        "                        conditions_support = support_count(conditions, transactions)\n",
        "                        if conditions_support > 0:\n",
        "                            confidence = support / conditions_support\n",
        "                            if confidence >= min_confidence:\n",
        "                                rules.append((conditions, results, support, confidence))  # Append the generated rules in the list.\n",
        "    return rules # Returns the list of rules generated by Brute Force Algorithm\n",
        "\n",
        "\n",
        "\n",
        "def display_frequent_itemsets(frequent_itemsets):  # Function to display the frequent itemsets with their support.\n",
        "    print(\"\\nFrequent Itemsets:\\n\")\n",
        "    print(\"-\" * 41)  # Separation line\n",
        "    print(\"{:<30} {:<10}\".format('|Itemset', ' |Support|'))\n",
        "    print(\"-\" * 41)  # Separation line\n",
        "    for index, row in frequent_itemsets.iterrows():\n",
        "        itemset = tuple(row['itemsets'])  # Get the itemset from the row\n",
        "        support = row['support']  # Get the support from the row\n",
        "        print(f\"|{str(itemset):<30} | {support * 100:.2f}%|\")  # Print itemset and support\n",
        "    print(\"-\" * 41)  # Separation line\n",
        "\n",
        "\n",
        "# Collecting all rules into sets for proper comparison\n",
        "def collect_rules(rules, method):\n",
        "    rule_set = set()\n",
        "    for rule in rules:\n",
        "        conditions, results = rule[0], rule[1]\n",
        "        rule_set.add(((tuple(sorted(conditions))), (tuple(sorted(results)))))  # Store rules as tuples of antecedents and consequents\n",
        "    return (rule_set)\n",
        "\n",
        "\n",
        "\n",
        "# Using dictionary to simulate switch-case for selecting datasets\n",
        "# Since, the datasets and script is located in same folder, so didn't required to put whole path . If any other dataset is being analyzed then need put whole path.\n",
        "dataset_files = {\n",
        "    1: './Datasets_CSV/amazon_csv.csv',\n",
        "    2: './Datasets_CSV/target_csv.csv',\n",
        "    3: './Datasets_CSV/walmart_csv.csv',\n",
        "    4: './Datasets_CSV/aldi_csv.csv',\n",
        "    5: './Datasets_CSV/costco_csv.csv',\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def convert_percentage(value):  # Function to convert percentage input of user to decimal for prcoessing.\n",
        "    return value / 100.0\n",
        "\n",
        "\n",
        "\n",
        "def main():  # Main function to run the algorithms and provide user interaction.\n",
        "    warnings.filterwarnings('ignore', category=DeprecationWarning) # Ignoring the depriciation warnings.\n",
        "    print(\"\\n\" + \"=\" * 100)  # Seperation line\n",
        "    print(\"\\nWelcome to the Association Rule Mining Program!\")\n",
        "\n",
        "    choice = \"y\"  # Initialize the choice variable\n",
        "    while choice.lower() != \"n\":\n",
        "        # Prompts for user inputs\n",
        "        print(\"\\n\"+\"=\" * 100) # Seperation line\n",
        "        print(\"=\" * 100)  # Seperation line\n",
        "        print(\"\\nChoose a supermarket for the analysis:\")\n",
        "        print(\"1. Amazon\")\n",
        "        print(\"2. Target\")\n",
        "        print(\"3. Walmart\")\n",
        "        print(\"4. Aldi\")\n",
        "        print(\"5. Costco\")\n",
        "        print(\"6. Provide a custom CSV dataset for the analysis\")\n",
        "        print(\"7. Exit from program\")\n",
        "\n",
        "        # Input and validation for the choice made by user.\n",
        "        try:\n",
        "            dataset_choice = int(input(\"\\nEnter your choice between 1 and 7: \"))\n",
        "            if dataset_choice not in range(1, 8):\n",
        "                print(\"\\n* Invalid choice. Please choose a number between 1 and 7.\\n\")\n",
        "                continue\n",
        "            elif dataset_choice == 7:  # Option to exit from program.\n",
        "                break\n",
        "        except ValueError:\n",
        "            print(\"\\n* Invalid input. Please enter a number.\\n\")\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        # Validation and loading the chosen dataset by user.\n",
        "        if dataset_choice in dataset_files:\n",
        "            transactions = dataset_loading(dataset_files[dataset_choice])\n",
        "        elif dataset_choice == 6:\n",
        "            custom_file = input(\"Enter the path of your custom CSV file: \")\n",
        "            transactions = dataset_loading(custom_file)\n",
        "        else:\n",
        "            print(\"\\n* Invalid choice. Please try again.\")\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        # Input and validation of the support and confidence\n",
        "        try:\n",
        "            if transactions == []:\n",
        "                print(\"\\n* Please try again.\")\n",
        "                continue\n",
        "            min_support_percent = float(input(\"Enter the minimum support percentage (1-100): \"))\n",
        "            if not (1 <= min_support_percent <= 100):\n",
        "                print(\"\\n* Support percentage must be between 1 and 100.\\n\")\n",
        "                continue\n",
        "            min_confidence_percent = float(input(\"Enter the minimum confidence percentage (1-100): \"))\n",
        "            if not (1 <= min_confidence_percent <= 100):\n",
        "                print(\"\\n* Confidence percentage must be between 1 and 100.\\n\")\n",
        "                continue\n",
        "        except ValueError:\n",
        "            print(\"\\n* Invalid input. Please enter numeric values.\\n\")\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        # Converting percentages to decimals.\n",
        "        min_support = convert_percentage(min_support_percent)\n",
        "        min_confidence = convert_percentage(min_confidence_percent)\n",
        "\n",
        "\n",
        "\n",
        "        # TransactionEncoder is used to convert transaction data (lists of items) into a one-hot encoded format. Each item becomes a column, with 1 indicating\n",
        "        # its presence in a transaction, and 0 indicating its absence.\n",
        "        encoder = TransactionEncoder()\n",
        "\n",
        "        # The fit method learns the unique items from the transaction data and transform method converts the transaction data into a one-hot encoded array.\n",
        "        onehot = encoder.fit(transactions).transform(transactions)\n",
        "\n",
        "        # Convert the one-hot encoded array into a DataFrame, where each column corresponds to an item from the original transactions, and the rows represent individual transactions.\n",
        "        data_frame = pd.DataFrame(onehot, columns=encoder.columns_)\n",
        "\n",
        "\n",
        "\n",
        "        # Displaying the frequent itemsets with their support.\n",
        "        print(\"\\n\"+\"=\" * 100)  # Seperation line\n",
        "        frequent_itemsets = apriori(data_frame, min_support=min_support, use_colnames=True)\n",
        "        if frequent_itemsets.empty:\n",
        "            print(\"\\n\\n* No frequent itemsets found with the specified support threshold.\\n* Please enter proper inputs again\")\n",
        "            continue\n",
        "        else:\n",
        "            display_frequent_itemsets(frequent_itemsets)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Measuring the execution time and generating rules by each algorithm.\n",
        "\n",
        "        # For Brute Force Algorithm\n",
        "        start_time = time.perf_counter()# using perf_counter() method for better accuracy.\n",
        "        rules_brute_force = brute_force_rules(transactions, min_support, min_confidence)  # Generates rules using Brute Force Algorithm.\n",
        "        if rules_brute_force == []:\n",
        "            print(\"\\n\\n* No rules found with the specified support and confidence.\\n* Please enter proper inputs again\")\n",
        "            continue\n",
        "        brute_force_time = time.perf_counter() - start_time  # Calculating execution time for Brute Force Algorithm.\n",
        "\n",
        "        # For Apriori Algorithm\n",
        "        start_time = time.perf_counter()\n",
        "        frequent_itemsets_apriori = apriori(data_frame, min_support=min_support, use_colnames=True)\n",
        "        if frequent_itemsets_apriori.empty:\n",
        "            print(\"\\n\\n* No frequent itemsets found with the specified support threshold.\\n* Please enter proper inputs again\")\n",
        "            continue\n",
        "        rules_apriori = association_rules(frequent_itemsets_apriori, metric=\"confidence\", min_threshold=min_confidence) # Generates rules using Apriori Algorithm.\n",
        "        apriori_time = time.perf_counter() - start_time  # Calculating execution time for Apriori Algorithm.\n",
        "\n",
        "        # For FP-Growth Algorithm\n",
        "        start_time = time.perf_counter()\n",
        "        frequent_itemsets_fp = fpgrowth(data_frame, min_support=min_support, use_colnames=True)\n",
        "        if frequent_itemsets_fp.empty:\n",
        "            print(\"\\n\\n* No frequent itemsets found with the specified support threshold.\\n* Please enter proper inputs again\")\n",
        "            continue\n",
        "        rules_fp_growth = association_rules(frequent_itemsets_fp, metric=\"confidence\", min_threshold=min_confidence)  # Generate rules using FP-Growth Algorithm.\n",
        "        fp_growth_time = time.perf_counter() - start_time  # Calculating execution time for FP-Growth Algorithm.\n",
        "\n",
        "\n",
        "\n",
        "        # Display rules for each algorithm.\n",
        "\n",
        "        # For Brute Force Algorithm\n",
        "        print(\"=\" * 100)  # Seperation line.\n",
        "        print(\"\\nRules generated using Brute Force Algorithm:\")\n",
        "        for index,rule in enumerate(rules_brute_force):\n",
        "            conditions, results, support, confidence = rule  # Unpack the rule for printing them.\n",
        "            conditions = (conditions)\n",
        "            results = (results)\n",
        "            print(f\"\\nRule {index+1}: {conditions} -> {results} has Support: {support * 100:.2f}% and Confidence: {confidence * 100:.2f}%\") # Formatted to print percentage of support and confidence rounded upto 2 decimals.\n",
        "\n",
        "        # For Apriori Algorithm\n",
        "        print(\"\\n\"+\"=\" * 100)  # Seperation line.\n",
        "        print(\"\\nRules generated using Apriori Algorithm:\")\n",
        "        for index, row in rules_apriori.iterrows():  # Iterating over DataFrame rows of onehot. Here index is a temporary variable.\n",
        "            conditions_apriori = (tuple(row['antecedents']))\n",
        "            results_apriori = (tuple(row['consequents']))\n",
        "            support = row['support']\n",
        "            confidence = row['confidence']\n",
        "            print(f\"\\nRule {index+1}: {conditions_apriori} -> {results_apriori} has Support: {support * 100:.2f}% and Confidence: {confidence * 100:.2f}%\") # Formatted to print percentage of support and confidence rounded upto 2 decimals.\n",
        "\n",
        "        # For FP-Growth Algorithm\n",
        "        print(\"\\n\"+\"=\" * 100)  # Seperation line.\n",
        "        print(\"\\nRules generated using FP-Growth Algorithm:\")\n",
        "        for index, row in rules_fp_growth.iterrows():  # Iterating over DataFrame rows of onehot. Here index is a temporary variable.\n",
        "            conditions_fp = (tuple(row['antecedents']))\n",
        "            results_fp = (tuple(row['consequents']))\n",
        "            support = row['support']\n",
        "            confidence = row['confidence']\n",
        "            print(f\"\\nRule {index+1}: {conditions_fp} -> {results_fp} has Support: {support * 100:.2f}% and Confidence: {confidence * 100:.2f}%\") # Formatted to print percentage of support and confidence rounded upto 2 decimals.\n",
        "\n",
        "\n",
        "\n",
        "        # Comparing the rules generated by algorithms.\n",
        "        # We used sets for comparison as in this order dosen't matter and also no duplicates are included.\n",
        "        # Collect rules for brute force, Apriori, and FP-Growth\n",
        "        brute_force_set = set(collect_rules(rules_brute_force, \"Brute Force\"))\n",
        "        apriori_set = set((tuple(sorted(row['antecedents'])), tuple(sorted(row['consequents']))) for index, row in rules_apriori.iterrows())\n",
        "        fp_growth_set = set((tuple(sorted(sorted(row['antecedents']))), tuple(sorted(row['consequents']))) for index, row in rules_fp_growth.iterrows())\n",
        "\n",
        "        # Perform set comparison\n",
        "        print(\"\\n\" + \"=\" * 100)  # Separation line\n",
        "        print(\"\\nComparison of Rules Generated by the Algorithms:\")\n",
        "        if brute_force_set == apriori_set == fp_growth_set:\n",
        "            print(\"\\nAll three algorithms generated the same rules.\")\n",
        "        else:\n",
        "            if brute_force_set == apriori_set:\n",
        "                print(\"\\nBrute Force and Apriori generated the same rules, but FP-Growth generated different rules.\")\n",
        "            elif brute_force_set == fp_growth_set:\n",
        "                print(\"\\nBrute Force and FP-Growth generated the same rules, but Apriori generated different rules.\")\n",
        "            elif apriori_set == fp_growth_set:\n",
        "                print(\"\\nApriori and FP-Growth generated the same rules, but Brute Force generated different rules.\")\n",
        "            else:\n",
        "                print(\"\\nAll three algorithms generated different rules.\")\n",
        "\n",
        "\n",
        "\n",
        "        # Comparing the execution time and determining the fastest, slowest, and intermediate algorithm.\n",
        "        times = {\n",
        "            \"Brute Force Algorithm\": brute_force_time,\n",
        "            \"Apriori Algorithm\": apriori_time,\n",
        "            \"FP-Growth Algorithm\": fp_growth_time,\n",
        "        }\n",
        "\n",
        "        # Sort the 'times' dictionary (which holds the algorithms and their execution times) by the execution time in ascending order. The lambda function 'x[1]' ensures\n",
        "        # sorting is based on the values (time), not the keys (algorithm names).\n",
        "        sorted_algorithms = sorted(times.items(), key=lambda x: x[1])\n",
        "        fastest = sorted_algorithms[0]\n",
        "        intermediate = sorted_algorithms[1]\n",
        "        slowest = sorted_algorithms[2]\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 100) # Seperation line.\n",
        "        print(\"\\nComparison of the execution times of algorithm:\")\n",
        "        print(f\"\\nThe Fastest algorithm is: {fastest[0]} with an execution time of {fastest[1]:.4f} seconds.\")\n",
        "        print(f\"\\nThe Intermediate algorithm is: {intermediate[0]} with an execution time of {intermediate[1]:.4f} seconds.\")\n",
        "        print(f\"\\nThe Slowest algorithm is: {slowest[0]} with an execution time of {slowest[1]:.4f} seconds.\") # Formatted to print the seconds upto 4 decimals with thier respective algorithms.\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 100) # Seperation line.\n",
        "        choice = input(\"\\nDo you want to run another analysis?\\nPress Y to continue or N to Exit: \")  # Choice to run the program until user wants.\n",
        "    print(\"\\n\" + \"=\" * 100) # Seperation line.\n",
        "    print(\"\\nThank you for using the Association Rule Mining Program! \\nHave a Great Day!\") #Greeting message.\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":  # Ensuring the main function runs when the script is executed.\n",
        "    main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}